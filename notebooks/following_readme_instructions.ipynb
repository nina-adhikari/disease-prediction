{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+iGAMsy9IawlJocJBFY1n"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides all the code from the readme in a single place for convenient use."
      ],
      "metadata": {
        "id": "TqFQ8eot_il1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS5fZKVV-hpt"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/nina-adhikari/disease_prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RLXSRxsE-tlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIRECTORY = 'drive/My Drive/Disease-Prediction/ddx-dataset/'"
      ],
      "metadata": {
        "id": "p2K1HvON-xzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification using the random forest model"
      ],
      "metadata": {
        "id": "wXB_JqR3_bVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from disease_prediction.data import datasets as ds\n",
        "\n",
        "# The datasets we want to load; you can choose fewer if you'd like\n",
        "SUBSETS = ['train', 'validate', 'test']\n",
        "\n",
        "df = ds.load_datasets(\n",
        "\t    subsets=SUBSETS,\n",
        "\t    directory=DIRECTORY\n",
        ")\n",
        "\n",
        "for subset in SUBSETS:\n",
        "\tdf[subset].set_index('index', inplace=True)"
      ],
      "metadata": {
        "id": "lyZxpaCv-8HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'Y': 1, 'N': 0}\n",
        "\n",
        "# drop the columns that have a single value in all three datasets and convert Y/N to 1/0\n",
        "for subset in SUBSETS:\n",
        "    df[subset].drop(columns=['pain_radiate', 'lesions_peeling'], inplace=True)\n",
        "    df[subset]['lesion_larger_than_1cm'] = df[subset]['lesion_larger_than_1cm'].map(d)"
      ],
      "metadata": {
        "id": "hhKBsZ-G_Cvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORICAL_FEATURES = [col for col in df['train'].columns if df['train'][col].dtype == 'object']\n",
        "CATEGORICAL_FEATURES.remove('PATHOLOGY')\n",
        "\n",
        "NUMERICAL_FEATURES = [col for col in df['train'].columns if (set(df['train'][col].unique()) != set([0,1])) and (df['train'][col].dtype != 'object')]\n",
        "\n",
        "X = {}\n",
        "y = {}\n",
        "\n",
        "for subset in SUBSETS:\n",
        "\tX[subset] = df[subset].drop(columns=['PATHOLOGY'])\n",
        "\ty[subset] = df[subset].PATHOLOGY.copy()"
      ],
      "metadata": {
        "id": "GVv20wju_OdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# optimal model hyperparameters\n",
        "n_estimators = 500\n",
        "max_depth = 20\n",
        "min_samples_leaf = 5\n",
        "bootstrap = False\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth,\n",
        "    min_samples_leaf=min_samples_leaf,\n",
        "    bootstrap=bootstrap\n",
        ")\n",
        "\n",
        "rf_pipeline = make_pipeline(\n",
        "    ColumnTransformer(\n",
        "\t    [\n",
        "\t    ('categorical', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES),\n",
        "\t    ('numerical', StandardScaler(), NUMERICAL_FEATURES)\n",
        "\t    ],\n",
        "\t    remainder='passthrough'),\n",
        "    rf\n",
        ")"
      ],
      "metadata": {
        "id": "L8_wRT0i_QfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_pipeline.fit(X['train'], y['train'])"
      ],
      "metadata": {
        "id": "PpMP5WeG_Ssu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y['validate'], rf_pipeline.predict(X['validate']), digits=4))\n",
        "print(classification_report(y['test'], rf_pipeline.predict(X['test']), digits=4))"
      ],
      "metadata": {
        "id": "opyt18OP_VNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification using the DistilBERT transformer"
      ],
      "metadata": {
        "id": "nLPl1npS_fMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers evaluate imblearn"
      ],
      "metadata": {
        "id": "I-AbdEfD_vPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from disease_prediction.models import text_classification as tc\n",
        "from disease_prediction.models import classification_helper as ch"
      ],
      "metadata": {
        "id": "507tQWQV_yWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = {}\n",
        "SUBSETS = ['train', 'validation', 'test']\n",
        "\n",
        "for subset in SUBSETS:\n",
        "\tdf[subset] = pd.read_json(DIRECTORY + 'text-' + subset + '-gpt.json')"
      ],
      "metadata": {
        "id": "7OKXWxoh_5LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_combined = pd.concat([df['train'], df['validation'], df['test']])\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_combined['sentence1'], df_combined['label'], test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "L_i2vM1DAPc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "\n",
        "X_resampled, y_resampled = rus.fit_resample(X_train.to_numpy().reshape(-1, 1), y_train.to_numpy())"
      ],
      "metadata": {
        "id": "QRqktrIuARi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_resampled = pd.DataFrame({'sentence1': X_resampled.reshape(-1), 'label': y_resampled})\n",
        "\n",
        "df['train'] = df_resampled\n",
        "df['validation'] = pd.DataFrame({'sentence1': X_test, 'label': y_test})\n",
        "df['test'] = pd.DataFrame({'sentence1': X_test, 'label': y_test})"
      ],
      "metadata": {
        "id": "NTpZFw5FATNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ch.DATA_ARGS.max_train_samples = len(X_resampled)\n",
        "ch.DATA_ARGS.max_val_samples = len(X_test)\n",
        "\n",
        "# This is not relevant since we are not predicting anything, but we define it anyway to placate the transformer\n",
        "ch.DATA_ARGS.max_test_samples = len(X_test)"
      ],
      "metadata": {
        "id": "6BTi0jreAViU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc.setup_from_scratch(df)"
      ],
      "metadata": {
        "id": "n2P5z1w7AjIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc.train()"
      ],
      "metadata": {
        "id": "zAkHsZBKAldL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc.evaluate()"
      ],
      "metadata": {
        "id": "0g1wXaGzAmpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc.WRAPPER.save_pretrained(DIRECTORY + 'model')"
      ],
      "metadata": {
        "id": "ez6gi762AufB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc.setup_from_finetuned(DIRECTORY + 'model')"
      ],
      "metadata": {
        "id": "nQ_BMAYRAvj5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}